{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ba78f5d",
   "metadata": {},
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b860dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"emnist-digits-train.csv\"\n",
    "df_train = pd.read_csv(input_file, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32f165",
   "metadata": {},
   "source": [
    "## 다운받은 데이터 numpy로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0500e206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0, 0, ..., 0, 0, 0],\n",
       "       [9, 0, 0, ..., 0, 0, 0],\n",
       "       [6, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = df_train.to_numpy()\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ec229",
   "metadata": {},
   "source": [
    "240000개의 데이터와 785개(타겟(1개), 픽셀데이터(784개))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e6585d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000, 785)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98348ba7",
   "metadata": {},
   "source": [
    "훈련셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba08de17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = data_train[:, 1:]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27239c09",
   "metadata": {},
   "source": [
    "타겟셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a01ebeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 9, 6, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = data_train[:, 0]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b31fe4",
   "metadata": {},
   "source": [
    "### 정규화는 이미지 프로세싱에서 자주 사용되는데, pixel intensities를 특정 범위(RGB 색상 범위의 경우 0~255) 내에 맞추려면 정규화가 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351c3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scale = scaler.fit_transform(X_train.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8242cb6f",
   "metadata": {},
   "source": [
    "### SGD (확률적 경사 하강법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2834d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# sgd_clf = SGDClassifier(max_iter=10, tol=1e-4, random_state=42)\n",
    "# sgd_clf.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01eed3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_scores = cross_val_score(sgd_clf, X_train, y_train, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa12780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d24ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean([0.92541667, 0.92775   , 0.92579167, 0.92625   , 0.92933333,\n",
    "#        0.929125  , 0.92858333, 0.93075   , 0.92820833, 0.92970833])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca241d3d",
   "metadata": {},
   "source": [
    "max_iter(10): 0.928091666"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ac026",
   "metadata": {},
   "source": [
    "### SVM 큰 데이터셋에 대해서는 성능이 좋은 편이 아니지만 그래도 한번 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd321b",
   "metadata": {},
   "source": [
    "SVM 알고리즘은 데이터를 정규화하는 것을 매우 추천한다, 하지만 앞에서 이미 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1ddb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "# svm_clf.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65020a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# svm_clf_cross_val_scores = cross_val_score(svm_clf, X_train, y_train, cv=10, scoring=\"accuracy\")\n",
    "# svm_clf_default_cross_val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581b951",
   "metadata": {},
   "source": [
    "SVM의 시간복잡도는 O(n^3)으로 굉장히 높기 때문에 Bagging을 통해 훈련 튜플의 수를 줄이고 병렬처리를 적용하면 시간 단축을 할 수 있다고 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ecfb406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# estimator = LinearSVC()\n",
    "# n_estimators = 10\n",
    "# n_jobs = 5\n",
    "# model = BaggingClassifier(estimator=estimator,\n",
    "#                           n_estimators=n_estimators,\n",
    "#                           max_samples=1./n_estimators,\n",
    "#                           n_jobs=n_jobs)\n",
    "# model.fit(X_train_scale,y_train)\n",
    "# model.predict(X_train_scale)\n",
    "# model.predict_proba(X_train_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30507410",
   "metadata": {},
   "source": [
    "* base_estimator는 LinearSVC뿐만 아니라 다른 분류 모델들도 적용가능하다.\n",
    "* n_estimators는 다수결을 수행할 estimator의 개수를 가리킨다.\n",
    "* max_samples는 각 estimator를 만들 때 전체 튜플의 몇 %를 사용할 지를 입력받는다.\n",
    "* n_jobs가 2이상인 경우가 병렬처리(multi-processing)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1aa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRFClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # import numpy as np\n",
    "\n",
    "# # # 데이터 로드 및 전처리\n",
    "# # # X_train, y_train = load_data()\n",
    "\n",
    "# # # 모델 생성\n",
    "# xgb_classifier = XGBRFClassifier()\n",
    "# lgbm_classifier = LGBMClassifier()\n",
    "# rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# # 하이퍼파라미터 튜닝 (예시로 일부 하이퍼파라미터만 설정)\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],\n",
    "#     'max_depth': [10, 20, 30]\n",
    "# }\n",
    "# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# rf_best = grid_search.best_estimator_\n",
    "\n",
    "# # 앙상블 모델 구성\n",
    "# model = [('LGBM', lgbm_classifier), ('RF', rf_best)]\n",
    "\n",
    "# voting_classifier = VotingClassifier(estimators=model, voting='hard', n_jobs=-1)\n",
    "\n",
    "# # 교차 검증을 사용한 성능 평가\n",
    "# cv_score = cross_val_score(voting_classifier, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "# print(f'Cross Validation Mean is {np.mean(cv_score)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccdb15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# svm_clf_linear_cross_val_scores = cross_val_score(model, X_train_scale, y_train, cv=10, scoring=\"accuracy\")\n",
    "# svm_clf_linear_cross_val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2012057",
   "metadata": {},
   "source": [
    "## XGB, LGBM, RF  그리드서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae812774",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRFClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# LGBMClassifier 및 RandomForestClassifier에 대한 그리드 서치를 수행할 매개변수 그리드 정의\n",
    "new_lgbm_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "new_rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# 그리드 서치를 사용하여 최적의 하이퍼파라미터 조합을 찾는다.\n",
    "new_lgbm_grid_search = GridSearchCV(LGBMClassifier(random_state=42), new_lgbm_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "new_lgbm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "new_rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), new_rf_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "new_rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델과 하이퍼파라미터 출력\n",
    "new_best_lgbm = new_lgbm_grid_search.best_estimator_\n",
    "new_best_rf = new_rf_grid_search.best_estimator_\n",
    "print(\"Best LGBM Classifier:\", new_best_lgbm)\n",
    "print(\"Best RandomForest Classifier:\", new_best_rf)\n",
    "\n",
    "# VotingClassifier를 업데이트하여 최적의 모델로 앙상블\n",
    "new_model = [('LGBM', new_best_lgbm), ('RF', new_best_rf)]\n",
    "new_votingC = VotingClassifier(estimators=new_model, voting='soft', n_jobs=-1)\n",
    "\n",
    "# 교차 검증을 사용한 성능 평가\n",
    "cv_score = cross_val_score(new_votingC, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "print(f'Cross Validation Mean is {np.mean(cv_score)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98b3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.9722916666666667\n",
      "Fold 2: 0.9711666666666666\n",
      "Fold 3: 0.9722916666666667\n",
      "Fold 4: 0.972\n",
      "Fold 5: 0.9720416666666667\n",
      "Fold 6: 0.9728333333333333\n",
      "Fold 7: 0.9735416666666666\n",
      "Fold 8: 0.9735833333333334\n",
      "Fold 9: 0.971375\n",
      "Fold 10: 0.9719583333333334\n",
      "Cross Validation Mean is 0.9723083333333333\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import numpy as np\n",
    "\n",
    "# # 모델 정의\n",
    "# extra_trees_classifier = ExtraTreesClassifier(n_estimators=100, max_depth=30, n_jobs=-1, random_state=42)\n",
    "# gradient_boost_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# # 앙상블 모델 생성\n",
    "# voting_classifier = VotingClassifier(estimators=[('etc', extra_trees_classifier), ('gbc', gradient_boost_classifier)], voting='soft')\n",
    "\n",
    "# # 교차 검증 수행\n",
    "# cv_scores = cross_val_score(voting_classifier, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "# # 각 폴드의 정확도 출력\n",
    "# for i, fold_scores in enumerate(cv_scores, start=1):\n",
    "#     print(f\"Fold {i}: {fold_scores}\")\n",
    "\n",
    "# # 평균 정확도 출력\n",
    "# print(f\"Cross Validation Mean is {np.mean(cv_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811dc2d",
   "metadata": {},
   "source": [
    "[0.92275   , 0.92379167, 0.92325   , 0.922875  , 0.92275   ,\n",
    "       0.92504167, 0.92458333, 0.92658333, 0.92429167, 0.92379167]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "729a8d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: 0.9902916666666667\n",
      "Fold 2: 0.9889166666666667\n",
      "Fold 3: 0.9902916666666667\n",
      "Fold 4: 0.9905\n",
      "Fold 5: 0.98875\n",
      "Fold 6: 0.9910416666666667\n",
      "Fold 7: 0.9903333333333333\n",
      "Fold 8: 0.9909166666666667\n",
      "Fold 9: 0.99025\n",
      "Fold 10: 0.9894583333333333\n",
      "Cross Validation Mean is 0.990075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 모델 정의\n",
    "extra_trees_classifier = ExtraTreesClassifier(n_estimators=100, max_depth=30, n_jobs=-1, random_state=42)\n",
    "poly_kernel_svm_classifier = SVC(kernel=\"poly\", degree=3, coef0=1, C=100, gamma='auto', max_iter=-1, probability=True)\n",
    "\n",
    "# 앙상블 모델 생성\n",
    "voting_classifier = VotingClassifier(estimators=[('etc', extra_trees_classifier), ('svc', poly_kernel_svm_classifier)], voting='soft')\n",
    "\n",
    "# 교차 검증 수행\n",
    "final_cv_scores = cross_val_score(voting_classifier, X_train, y_train, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "\n",
    "# 각 폴드의 정확도 출력\n",
    "for i, score in enumerate(final_cv_scores, start=1):\n",
    "    print(f\"Fold {i}: {score}\")\n",
    "\n",
    "# 평균 정확도 출력\n",
    "print(f\"Cross Validation Mean is {np.mean(final_cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663dbfcf",
   "metadata": {},
   "source": [
    "##### 0.9900666666666667"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
