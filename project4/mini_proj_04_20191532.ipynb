{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"winequality-red.csv\"\n",
    "data = pd.read_csv(input_file)\n",
    "tmp = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 부족으로 판단하여 2배 증가 X (불가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([data,data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"residual sugar\", axis=1)\n",
    "data = data.drop(\"density\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0.124052\n",
       "volatile acidity       -0.390558\n",
       "citric acid             0.226373\n",
       "chlorides              -0.128907\n",
       "free sulfur dioxide    -0.050656\n",
       "total sulfur dioxide   -0.185100\n",
       "pH                     -0.057731\n",
       "sulphates               0.251397\n",
       "alcohol                 0.476166\n",
       "quality                 1.000000\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "\n",
    "# # features = housing_train[housing_train.drop('price', axis=1).columns]\n",
    "# # housing_train = housing_train[['price','sqft_living//waterfront','(waterfront//(waterfront/sqft_living))','(sqft_living15*((sqft_living**2)//(waterfront//(waterfront/sqft_living))))','(waterfront//waterfront/sqft_living)','(sqft_living15*(waterfront//(waterfront/sqft_living)))']]\n",
    "# features = data.drop('quality', axis=1)\n",
    "\n",
    "# combinations_list = list(combinations(features.columns, 2))\n",
    "# # # 조합된 특성들의 평균을 나타내는 새로운 특성들 생성\n",
    "# for combo in combinations_list:\n",
    "#     print(\"1Number of columns in housing_train:\", len(data.columns))\n",
    "#     print(f\"0Number of columns in housing_train:\", len(data.columns), f\"{combo}\")\n",
    "#     data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}*{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] * features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}**{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] ** features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}//{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] // features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}+2{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] + 2*features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"(2{combo[0]}+{combo[1]})\"\n",
    "#     data[feature_name3] = (2*features[combo[0]] + features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"(({combo[0]}+{combo[1]})/2)\"\n",
    "#     data[feature_name3] = (features[combo[0]] + features[combo[1]])/2\n",
    "\n",
    "# # features = data.drop('quality', axis=1)\n",
    "\n",
    "# # combinations_list = list(combinations(features.columns, 2))\n",
    "# # # # 조합된 특성들의 평균을 나타내는 새로운 특성들 생성\n",
    "# # for combo in combinations_list:\n",
    "# #     print(\"1Number of columns in housing_train:\", len(data.columns))\n",
    "# #     print(f\"0Number of columns in housing_train:\", len(data.columns), f\"{combo}\")\n",
    "# #     data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# #     feature_name3 = f\"({combo[0]}*{combo[1]})\"\n",
    "# #     data[feature_name3] = (features[combo[0]] * features[combo[1]])\n",
    "\n",
    "# #     feature_name3 = f\"({combo[0]}**{combo[1]})\"\n",
    "# #     data[feature_name3] = (features[combo[0]] ** features[combo[1]])\n",
    "\n",
    "# #     feature_name3 = f\"({combo[0]}//{combo[1]})\"\n",
    "# #     data[feature_name3] = (features[combo[0]] // features[combo[1]])\n",
    "\n",
    "# #     feature_name3 = f\"({combo[0]}+2{combo[1]})\"\n",
    "# #     data[feature_name3] = (features[combo[0]] + 2*features[combo[1]])\n",
    "\n",
    "# #     feature_name3 = f\"(2{combo[0]}+{combo[1]})\"\n",
    "# #     data[feature_name3] = (2*features[combo[0]] + features[combo[1]])\n",
    "\n",
    "# #     feature_name3 = f\"(({combo[0]}+{combo[1]})/2)\"\n",
    "# #     data[feature_name3] = (features[combo[0]] + features[combo[1]])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = data.corr()\n",
    "# plt.figure(figsize=(20, 9))\n",
    "# k = 12 #number of variables for heatmap\n",
    "# cols = corr.nlargest(k, 'quality')['quality'].index\n",
    "# cm = np.corrcoef(data[cols].values.T)\n",
    "# sns.set(font_scale=1.25)\n",
    "# hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap=\"Blues\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional calculations\n",
    "data['total_acidity'] = data['fixed acidity'] + data['volatile acidity']\n",
    "data['average_acidity'] = (data['fixed acidity'] + data['volatile acidity']) / 2\n",
    "data['total_minerals'] = data['chlorides'] + data['sulphates']\n",
    "data['average_minerals'] = (data['chlorides'] + data['sulphates']) / 2\n",
    "data['non_free_sulfur_dioxide'] = np.abs(data['total sulfur dioxide'] - data['free sulfur dioxide'])\n",
    "\n",
    "# calculate percentage_free_sulfur without using CombineWithReferenceFeature\n",
    "data['percentage_free_sulfur'] = data['free sulfur dioxide'] / data['total sulfur dioxide']\n",
    "data['percentage_free_sulfur'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data['percentage_free_sulfur'].fillna(0, inplace=True)  # handling possible NaN values\n",
    "\n",
    "# additional calculations\n",
    "data['percentage_salt_sulfur'] = data['sulphates'] / data['free sulfur dioxide']\n",
    "data['percentage_salt_sulfur'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data['percentage_salt_sulfur'].fillna(0, inplace=True)  # handling possible NaN values\n",
    "\n",
    "# data['alcohol+sulphates'] = data['alcohol'] + data['sulphates']\n",
    "\n",
    "data[\"balance\"] = data[\"alcohol\"] - data[\"sulphates\"] - data[\"volatile acidity\"]\n",
    "data['total_minerals'] = data['chlorides'] + data['sulphates']\n",
    "data['sulph_sub_chlor']= data['sulphates']- data['chlorides']\n",
    "\n",
    "# data['(2sulphates+alcohol)'] = 2*data['sulphates'] + data['alcohol']\n",
    "data['(2citric acid+alcohol)'] = 2*data['citric acid'] + data['alcohol']\n",
    "# data['((sulphates+alcohol)/2)'] = ((data['sulphates']+data['alcohol'])/2)\n",
    "# (2sulphates+alcohol)                           0.516197\n",
    "# (2citric acid+alcohol)                         0.507274 X\n",
    "# ((sulphates+alcohol)/2)                        0.502502 X\n",
    "\n",
    "\n",
    "data['1'] = data['balance'] + 2*data['sulph_sub_chlor']\n",
    "# (balance+2sulph_sub_chlor)                          0.540696\n",
    "# (2sulphates+balance)                                0.538952 X\n",
    "# (2sulph_sub_chlor+(2citric acid+alcohol))           0.538115 X \n",
    "\n",
    "data['2'] = data['volatile acidity']**data['sulph_sub_chlor']\n",
    "# (volatile acidity**sulph_sub_chlor)             -0.446816\n",
    "\n",
    "data['3'] = (data['alcohol']/data['1'])\n",
    "# (alcohol/1)                                  -0.404521\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality                    1.000000\n",
       "1                          0.540696\n",
       "(2citric acid+alcohol)     0.507274\n",
       "balance                    0.483355\n",
       "alcohol                    0.476166\n",
       "sulph_sub_chlor            0.307735\n",
       "sulphates                  0.251397\n",
       "citric acid                0.226373\n",
       "percentage_free_sulfur     0.194113\n",
       "average_minerals           0.190327\n",
       "total_minerals             0.190327\n",
       "percentage_salt_sulfur     0.128885\n",
       "fixed acidity              0.124052\n",
       "average_acidity            0.085709\n",
       "total_acidity              0.085709\n",
       "free sulfur dioxide       -0.050656\n",
       "pH                        -0.057731\n",
       "chlorides                 -0.128907\n",
       "total sulfur dioxide      -0.185100\n",
       "non_free_sulfur_dioxide   -0.205463\n",
       "volatile acidity          -0.390558\n",
       "2                         -0.446816\n",
       "3                         -0.448255\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "\n",
    "# # features = housing_train[housing_train.drop('price', axis=1).columns]\n",
    "# # housing_train = housing_train[['price','sqft_living//waterfront','(waterfront//(waterfront/sqft_living))','(sqft_living15*((sqft_living**2)//(waterfront//(waterfront/sqft_living))))','(waterfront//waterfront/sqft_living)','(sqft_living15*(waterfront//(waterfront/sqft_living)))']]\n",
    "# features = data.drop('quality', axis=1)\n",
    "\n",
    "# combinations_list = list(combinations(features.columns, 2))\n",
    "# # # 조합된 특성들의 평균을 나타내는 새로운 특성들 생성\n",
    "# for combo in combinations_list:\n",
    "#     print(\"1Number of columns in housing_train:\", len(data.columns))\n",
    "#     print(f\"0Number of columns in housing_train:\", len(data.columns), f\"{combo}\")\n",
    "#     data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}*{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] * features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}**{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] ** features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}/{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] // features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"({combo[0]}+2{combo[1]})\"\n",
    "#     data[feature_name3] = (features[combo[0]] + 2*features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"(2{combo[0]}+{combo[1]})\"\n",
    "#     data[feature_name3] = (2*features[combo[0]] + features[combo[1]])\n",
    "\n",
    "#     feature_name3 = f\"(({combo[0]}+{combo[1]})/2)\"\n",
    "#     data[feature_name3] = (features[combo[0]] + features[combo[1]])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality                    1.000000\n",
       "1                          0.540696\n",
       "(2citric acid+alcohol)     0.507274\n",
       "balance                    0.483355\n",
       "alcohol                    0.476166\n",
       "sulph_sub_chlor            0.307735\n",
       "sulphates                  0.251397\n",
       "citric acid                0.226373\n",
       "percentage_free_sulfur     0.194113\n",
       "average_minerals           0.190327\n",
       "total_minerals             0.190327\n",
       "percentage_salt_sulfur     0.128885\n",
       "fixed acidity              0.124052\n",
       "average_acidity            0.085709\n",
       "total_acidity              0.085709\n",
       "free sulfur dioxide       -0.050656\n",
       "pH                        -0.057731\n",
       "chlorides                 -0.128907\n",
       "total sulfur dioxide      -0.185100\n",
       "non_free_sulfur_dioxide   -0.205463\n",
       "volatile acidity          -0.390558\n",
       "2                         -0.446816\n",
       "3                         -0.448255\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3                         -0.448255\n",
       "2                         -0.446816\n",
       "volatile acidity          -0.390558\n",
       "non_free_sulfur_dioxide   -0.205463\n",
       "total sulfur dioxide      -0.185100\n",
       "chlorides                 -0.128907\n",
       "pH                        -0.057731\n",
       "free sulfur dioxide       -0.050656\n",
       "total_acidity              0.085709\n",
       "average_acidity            0.085709\n",
       "fixed acidity              0.124052\n",
       "percentage_salt_sulfur     0.128885\n",
       "total_minerals             0.190327\n",
       "average_minerals           0.190327\n",
       "percentage_free_sulfur     0.194113\n",
       "citric acid                0.226373\n",
       "sulphates                  0.251397\n",
       "sulph_sub_chlor            0.307735\n",
       "alcohol                    0.476166\n",
       "balance                    0.483355\n",
       "(2citric acid+alcohol)     0.507274\n",
       "1                          0.540696\n",
       "quality                    1.000000\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix['quality'].sort_values(ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.drop('quality', axis=1)\n",
    "train_data = train_data.dropna(axis=1)\n",
    "train_target = data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_data)\n",
    "\n",
    "train_data = scaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# extra_trees_clf = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',\n",
    "#                     max_depth=30, max_features=1.0, max_leaf_nodes=None,\n",
    "#                     max_samples=None, min_impurity_decrease=0.0,\n",
    "#                     min_samples_leaf=1, min_samples_split=2,\n",
    "#                     min_weight_fraction_leaf=0.0, n_estimators=605, n_jobs=-1,\n",
    "#                     oob_score=False, random_state=42, verbose=0,\n",
    "#                     warm_start=False)\n",
    "\n",
    "\n",
    "# # lr = LogisticRegression()\n",
    "# for i,e in enumerate(train_data.columns):\n",
    "#     extra_trees_clf.fit(train_data[e].values[:,np.newaxis], train_target.values)\n",
    "#     plt.title(\"Best fit line\")\n",
    "#     plt.xlabel(str(e))\n",
    "#     plt.ylabel('quality')\n",
    "#     plt.scatter(train_data[e].values[:,np.newaxis], train_target)\n",
    "#     plt.plot(train_data[e].values[:,np.newaxis], extra_trees_clf.predict(train_data[e].values[:,np.newaxis]),color='r')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# sh_in = np.random.permutation(len(train_data))\n",
    "# train_data = train_data.values[sh_in]\n",
    "# train_target = train_target.values[sh_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미 방향성을 지니고 있기 때문에 PCA는 적용하지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # RandomForest 모델 생성 및 학습\n",
    "# rf_regressor = RandomForestRegressor(random_state=42)\n",
    "# rf_regressor.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# # Ada Boosting 모델 생성 및 학습\n",
    "# ada_regressor = AdaBoostRegressor(random_state=42)\n",
    "# ada_regressor.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-폴드 CV의 평균을 계산하기 위한 함수\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# train_data = poly_features.fit_transform(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.608943076739518,\n",
       " array([-1.36046815e+12,  8.96767726e+11,  2.21653230e+12,  2.47192708e+12,\n",
       "         3.22967662e+11, -1.01567558e+12, -6.60404429e-02,  3.52158536e+13,\n",
       "        -1.06964220e+11,  1.75471548e+11,  1.15604673e+12, -1.26455066e+13,\n",
       "        -1.17876607e+13,  8.35389286e+11,  4.96678482e-02,  5.91799789e-03,\n",
       "         2.58672564e+12, -1.29122215e+13, -6.68016449e+12,  4.03507061e+12,\n",
       "        -1.39097342e-02,  3.75778769e-01]))"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_data, train_target)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_learning_curves(lin_reg, train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52835961,  0.96187667, -1.39147228, ..., -1.0869892 ,\n",
       "         0.99087692,  0.97277383],\n",
       "       [-0.29854743,  1.96744245, -1.39147228, ..., -0.83371914,\n",
       "         1.53112407,  1.34439663],\n",
       "       [-0.29854743,  1.29706527, -1.18607043, ..., -0.74644365,\n",
       "         1.09391486,  0.93091944],\n",
       "       ...,\n",
       "       [-1.1603431 , -0.09955388, -0.72391627, ...,  0.60718214,\n",
       "        -0.29211499, -0.50618085],\n",
       "       [-1.39015528,  0.65462046, -0.77526673, ..., -0.22535761,\n",
       "         0.46531067,  0.09802417],\n",
       "       [-1.33270223, -1.21684919,  1.02199944, ...,  0.71670433,\n",
       "        -1.13725942, -0.92482707]])"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_trees_clf = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',\n",
    "#                     max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "#                     max_samples=None, min_impurity_decrease=0.0,\n",
    "#                     min_samples_leaf=1, min_samples_split=2,\n",
    "#                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "#                     oob_score=False, random_state=123, verbose=0,\n",
    "#                     warm_start=False)\n",
    "# extra_trees_clf.fit(train_data, train_target)\n",
    "# extra_trees_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.ensemble import StackingRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # 기본 모델 정의\n",
    "# base_models = [\n",
    "#     # ('linear', LinearRegression()),\n",
    "#     ('rf',RandomForestRegressor(n_estimators=10, random_state=42)),\n",
    "#     # ('tree', DecisionTreeRegressor()),\n",
    "#     # ('et', ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',\n",
    "#     #                 max_depth=30, max_features=1.0, max_leaf_nodes=None,\n",
    "#     #                 max_samples=None, min_impurity_decrease=0.0,\n",
    "#     #                 min_samples_leaf=1, min_samples_split=2,\n",
    "#     #                 min_weight_fraction_leaf=0.0, n_estimators=605, n_jobs=-1,\n",
    "#     #                 oob_score=False, random_state=42, verbose=0,\n",
    "#     #                 warm_start=False)),\n",
    "#     ('xgb',XGBRegressor()),\n",
    "#     ('hist',HistGradientBoostingRegressor())\n",
    "#     # ('knn', KNeighborsRegressor()),\n",
    "#     # ('svr', SVR()),\n",
    "#     # ('gradient_boosting', GradientBoostingRegressor())\n",
    "# ]\n",
    "\n",
    "# # 메타 모델 정의\n",
    "# # meta_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "# meta_model = SVR()\n",
    "# # 스태킹 모델 정의\n",
    "# stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# # 각 모델 훈련 및 예측\n",
    "# for name, model in base_models + [('stacking', stacking_model)]:\n",
    "#     model.fit(train_data, train_target)\n",
    "#     # y_pred = model.predict(train_data)\n",
    "#     # mse = mean_squared_error(y_val, y_pred)\n",
    "#     # print(f\"{name} MSE: {mse}\")\n",
    "\n",
    "# # 최종 스태킹 모델 훈련\n",
    "# stacking_model.fit(train_data, train_target)\n",
    "\n",
    "# scores = cross_val_score(stacking_model, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# extra_trees_clf = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='squared_error',\n",
    "#                     max_depth=30, max_features=1.0, max_leaf_nodes=None,\n",
    "#                     max_samples=None, min_impurity_decrease=0.0,\n",
    "#                     min_samples_leaf=1, min_samples_split=2,\n",
    "#                     min_weight_fraction_leaf=0.0, n_estimators=605, n_jobs=-1,\n",
    "#                     oob_score=False, random_state=42, verbose=0,\n",
    "#                     warm_start=False)\n",
    "\n",
    "# scores = cross_val_score(extra_trees_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores: [0.63246459 0.59018243 0.69025347 0.67308891 0.56489891 0.73273246\n",
    " 0.66521203 0.64682767 0.60001301 0.69950872]\n",
    "Mean: 0.6495182190019155\n",
    "Standard deviation: 0.05031390577785743"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_trees_clf = ExtraTreesRegressor(max_depth = 30, max_features = 4, min_samples_leaf = 1, min_samples_split = 2,\n",
    "#                                       n_estimators =2000, random_state=42)\n",
    "# scores = cross_val_score(extra_trees_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores: [0.63397486 0.59288826 0.66513791 0.66530967 0.55530522 0.7261735\n",
    " 0.64796038 0.6461098  0.59657687 0.68522293]\n",
    "Mean: 0.6414659391283845\n",
    "Standard deviation: 0.04705027069060762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5525433647824474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# svr_clf = LinearSVR()\n",
    "\n",
    "# scores = cross_val_score(svr_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores: [0.68280167 0.64441914 0.67532206 0.66701238 0.61949657 0.72280197\n",
    " 0.64714322 0.65848849 0.6057777  0.70139901]\n",
    "Mean: 0.6624662204640509\n",
    "Standard deviation: 0.03375193215292174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6331656216936654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# tree_reg = DecisionTreeRegressor(max_depth=3)\n",
    "# tree_reg.fit(train_data, train_target)\n",
    "\n",
    "# scores = cross_val_score(tree_reg, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# voting_clf = VotingRegressor(estimators=[('Linear Regression', LinearRegression(n_jobs=-1)),\n",
    "#                             ('Random Forest Regressor',\n",
    "#                              RandomForestRegressor(n_jobs=-1,\n",
    "#                                                    random_state=123))],\n",
    "#                 n_jobs=-1)\n",
    "# voting_clf.fit(train_data, train_target)\n",
    "\n",
    "# scores = cross_val_score(voting_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6304569173923081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# voting_clf = VotingRegressor(estimators=[('Linear Regression', LinearRegression(n_jobs=-1)),\n",
    "#                             ('Random Forest Regressor',\n",
    "#                              RandomForestRegressor(n_jobs=-1,\n",
    "#                                                    random_state=123)),\n",
    "#                             ('Extra Trees Regressor',\n",
    "#                              ExtraTreesRegressor(n_jobs=-1, random_state=123)),\n",
    "#                             ('Extreme Gradient Boosting',\n",
    "#                              XGBRegressor(base_score=None, booster='gbtree',\n",
    "#                                           callbacks=None,\n",
    "#                                           colsample_bylevel=None,\n",
    "#                                           max_delta_step=None, max_depth=None,\n",
    "#                                           max_leaves=None,\n",
    "#                                           min_child_weight=None,\n",
    "#                                           monotone_constraints=None,\n",
    "#                                           multi_strategy=None,\n",
    "#                                           n_estimators=None, n_jobs=-1,\n",
    "#                                           num_parallel_tree=None,\n",
    "#                                           random_state=123)),\n",
    "#                             ('Gradient Boosting Regressor',\n",
    "#                              GradientBoostingRegressor(random_state=123)),\n",
    "#                             # ('Ridge Regression', Ridge(random_state=123)),\n",
    "#                             # ('Bayesian Ridge', BayesianRidge())],\n",
    "#                 ],n_jobs=-1)\n",
    "# scores = cross_val_score(voting_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6249271589107086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_clf = VotingRegressor(estimators=[('Linear Regression', LinearRegression(n_jobs=-1)),\n",
    "#                             ('Random Forest Regressor',\n",
    "#                              RandomForestRegressor(n_jobs=-1,\n",
    "#                                                    random_state=123)),\n",
    "#                             # ('Extra Trees Regressor',\n",
    "#                             #  ExtraTreesRegressor(n_jobs=-1, random_state=123)),\n",
    "#                             ('Extreme Gradient Boosting',\n",
    "#                              XGBRegressor(base_score=None, booster='gbtree',\n",
    "#                                           callbacks=None,\n",
    "#                                           colsample_bylevel=None,\n",
    "#                                           max_leaves=None,\n",
    "#                                           min_child_weight=None, \n",
    "#                                           monotone_constraints=None,\n",
    "#                                           multi_strategy=None,\n",
    "#                                           n_estimators=None, n_jobs=-1,\n",
    "#                                           num_parallel_tree=None,\n",
    "#                                           random_state=123)),\n",
    "#                             ('Gradient Boosting Regressor',\n",
    "#                              GradientBoostingRegressor(random_state=123)),\n",
    "#                             # ('Light Gradient Boosting Machine',\n",
    "#                             #  LGBMRegressor(random_state=123)),\n",
    "#                             ],\n",
    "#                 n_jobs=-1)\n",
    "\n",
    "# scores = cross_val_score(voting_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6277047020206654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_clf = VotingRegressor(estimators=[('Linear Regression', LinearRegression(n_jobs=-1)),\n",
    "#                             # ('Random Forest Regressor',\n",
    "#                             #  RandomForestRegressor(n_jobs=-1,\n",
    "#                             #                        random_state=123)),\n",
    "#                             # ('Extra Trees Regressor',\n",
    "#                             #  ExtraTreesRegressor(n_jobs=-1, random_state=123)),\n",
    "#                             # ('Extreme Gradient Boosting',\n",
    "#                             #  XGBRegressor(base_score=None, booster='gbtree',\n",
    "#                             #               callbacks=None,\n",
    "#                             #               colsample_bylevel=None,\n",
    "#                             #               max_leaves=None,\n",
    "#                             #               min_child_weight=None, \n",
    "#                             #               monotone_constraints=None,\n",
    "#                             #               multi_strategy=None,\n",
    "#                             #               n_estimators=None, n_jobs=-1,\n",
    "#                             #               num_parallel_tree=None,\n",
    "#                             #               random_state=123)),\n",
    "#                             ('Gradient Boosting Regressor',\n",
    "#                              GradientBoostingRegressor(random_state=123)),\n",
    "#                             # ('Light Gradient Boosting Machine',\n",
    "#                             #  LGBMRegressor(random_state=123)),\n",
    "#                             ],\n",
    "#                 n_jobs=-1)\n",
    "\n",
    "# scores = cross_val_score(voting_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6224548184464493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "# voting_clf = VotingRegressor(estimators=[('Linear Regression', LinearRegression(n_jobs=-1)),\n",
    "#                                          ('svr', SVR()),\n",
    "#                             ('Gradient Boosting Regressor',\n",
    "#                              GradientBoostingRegressor(random_state=123)),\n",
    "#                             ],\n",
    "#                 n_jobs=-1)\n",
    "\n",
    "# scores = cross_val_score(voting_clf, train_data, train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6182150233167303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 너무 오래걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# seed = 553419\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# # Assuming train_data and train_target are your input data and target variable\n",
    "# shuffled_indices = np.random.permutation(len(train_data))\n",
    "# shuffled_data = train_data[shuffled_indices]\n",
    "# shuffled_target = train_target.values[shuffled_indices]\n",
    "\n",
    "# # Hyperparameter grid for ExtraTreesRegressor\n",
    "# param_grid = {\n",
    "#     'n_estimators': range(1000, 3000, 100),\n",
    "#     'max_depth': [29,30,31,32],\n",
    "#     'max_features': [3,4,5],\n",
    "#     'min_samples_leaf': [1,2],\n",
    "#     'min_samples_split': [2],\n",
    "#     'random_state': [42],\n",
    "# }\n",
    "# # Initialize ExtraTreesRegressor\n",
    "# extra_trees_clf = ExtraTreesRegressor()\n",
    "\n",
    "# # GridSearchCV to find the best hyperparameters\n",
    "# grid_search = GridSearchCV(estimator=extra_trees_clf, param_grid=param_grid, scoring='neg_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# grid_search.fit(shuffled_data, shuffled_target)\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Extract the best model with the optimal hyperparameters\n",
    "# best_extra_trees_clf = grid_search.best_estimator_\n",
    "\n",
    "# # Cross-validation with the best model\n",
    "# scores = cross_val_score(best_extra_trees_clf, shuffled_data, shuffled_target, scoring='neg_mean_squared_error', cv=10, n_jobs=-1)\n",
    "# rmse_scores = np.sqrt(-scores)\n",
    "# display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import ExtraTreesRegressor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# seed = 553419\n",
    "# np.random.seed(seed)\n",
    "# shuffled_in = np.random.permutation(len(train_data))\n",
    "# shuffled_train_data = train_data[shuffled_in]\n",
    "# shuffled_train_target = train_target[shuffled_in]\n",
    "# for i in range(1000, 10000, 100):\n",
    "#     extra_trees_clf = ExtraTreesRegressor(max_depth = 30, max_features = 3, min_samples_leaf = 1, min_samples_split = 2,\n",
    "#                                         n_estimators = i, random_state=42)\n",
    "\n",
    "#     scores = cross_val_score(extra_trees_clf, sh_t_data, sh_t_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "#     rmse_scores = np.sqrt(-scores)\n",
    "#     print(f\"현재값 {i}\")\n",
    "#     display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.53273315 0.59300028 0.51516492 0.52162257 0.56635211 0.6256167\n",
      " 0.46653252 0.5009357  0.53848157 0.4908538 ]\n",
      "Mean: 0.535129330882258\n",
      "Standard deviation: 0.045765609946900425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "seed = 553419\n",
    "np.random.seed(seed)\n",
    "shuffled_in = np.random.permutation(len(train_data))\n",
    "shuffled_train_data = train_data[shuffled_in]\n",
    "shuffled_train_target = train_target[shuffled_in]\n",
    "\n",
    "extra_trees_clf = ExtraTreesRegressor(max_depth = 30, max_features = 3, min_samples_leaf = 1, min_samples_split = 2,\n",
    "                                      n_estimators =1100, random_state=42)\n",
    "\n",
    "scores = cross_val_score(extra_trees_clf, shuffled_train_data, shuffled_train_target, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "display_scores(rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 0.5351887017868129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
